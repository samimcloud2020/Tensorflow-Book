{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**How to select best model**\n",
        "1. train multiple model with different hyperparameter on train set.\n",
        "\n",
        "2. evaluate each model's on validation set.\n",
        "\n",
        "3. select best model.\n",
        "\n",
        "4. train best model again with train set + validation set.\n",
        "\n",
        "5. got final model.\n",
        "\n",
        "6. evaluate with test set.\n",
        "\n",
        "."
      ],
      "metadata": {
        "id": "ncgU21yEIgLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "\n",
        "# --- 1. Load and Prepare Data (Using MNIST as a nice dataset) ---\n",
        "print(\"--- 1. Loading and preparing the MNIST dataset ---\")\n",
        "(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train_full = x_train_full.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Reshape data for CNN input (add channel dimension)\n",
        "x_train_full = np.expand_dims(x_train_full, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "# Split full training data into training and validation sets\n",
        "# Using 10,000 samples for validation\n",
        "x_train, x_val = x_train_full[:-10000], x_train_full[-10000:]\n",
        "y_train, y_val = y_train_full[:-10000], y_train_full[-10000:]\n",
        "\n",
        "print(f\"Training data shape: {x_train.shape}, labels shape: {y_train.shape}\")\n",
        "print(f\"Validation data shape: {x_val.shape}, labels shape: {y_val.shape}\")\n",
        "print(f\"Test data shape: {x_test.shape}, labels shape: {y_test.shape}\")\n",
        "\n",
        "# --- 2. Define the Model Architecture ---\n",
        "def build_model(learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Builds a simple Convolutional Neural Network (CNN) model.\n",
        "    Args:\n",
        "        learning_rate (float): The learning rate for the Adam optimizer.\n",
        "    Returns:\n",
        "        keras.Model: The compiled Keras model.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(10, activation=\"softmax\"),\n",
        "    ])\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# --- 3. Train Multiple Models with Different Hyperparameters ---\n",
        "print(\"\\n--- 3. Training multiple models with different hyperparameters ---\")\n",
        "\n",
        "# Define hyperparameter combinations to test\n",
        "hyperparameters = [\n",
        "    {\"learning_rate\": 0.01, \"batch_size\": 32, \"epochs\": 5},\n",
        "    {\"learning_rate\": 0.001, \"batch_size\": 32, \"epochs\": 5},\n",
        "    {\"learning_rate\": 0.0005, \"batch_size\": 64, \"epochs\": 5},\n",
        "    {\"learning_rate\": 0.001, \"batch_size\": 64, \"epochs\": 5},\n",
        "]\n",
        "\n",
        "models = []\n",
        "histories = []\n",
        "validation_accuracies = []\n",
        "\n",
        "for i, params in enumerate(hyperparameters):\n",
        "    print(f\"\\n--- Training Model {i+1}/{len(hyperparameters)} ---\")\n",
        "    print(f\"Hyperparameters: {params}\")\n",
        "\n",
        "    # Build a new model for each hyperparameter set\n",
        "    model = build_model(learning_rate=params[\"learning_rate\"])\n",
        "\n",
        "    # Train the model on the training set\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=params[\"batch_size\"],\n",
        "                        epochs=params[\"epochs\"],\n",
        "                        verbose=0) # Set verbose to 1 to see training progress per epoch\n",
        "\n",
        "    models.append(model)\n",
        "    histories.append(history)\n",
        "\n",
        "    # --- 4. Evaluate Each Model on the Validation Set ---\n",
        "    print(f\"--- 4. Evaluating Model {i+1} on the validation set ---\")\n",
        "    val_loss, val_accuracy = model.evaluate(x_val, y_val, verbose=0)\n",
        "    validation_accuracies.append(val_accuracy)\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "# --- 5. Select the Best Model ---\n",
        "print(\"\\n--- 5. Selecting the best model ---\")\n",
        "best_model_index = np.argmax(validation_accuracies)\n",
        "best_val_accuracy = validation_accuracies[best_model_index]\n",
        "best_params = hyperparameters[best_model_index]\n",
        "\n",
        "print(f\"Best Model Index: {best_model_index + 1}\")\n",
        "print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# --- 6. Train Best Model Again with Train Set + Validation Set ---\n",
        "print(\"\\n--- 6. Retraining the best model on the combined train and validation set ---\")\n",
        "\n",
        "# Combine training and validation sets\n",
        "x_train_combined = np.concatenate((x_train, x_val), axis=0)\n",
        "y_train_combined = np.concatenate((y_train, y_val), axis=0)\n",
        "\n",
        "print(f\"Combined training data shape: {x_train_combined.shape}, labels shape: {y_train_combined.shape}\")\n",
        "\n",
        "# Build a new model instance with the best hyperparameters\n",
        "final_model = build_model(learning_rate=best_params[\"learning_rate\"])\n",
        "\n",
        "# Train the final model\n",
        "# Using more epochs for the final training, as it's on a larger dataset\n",
        "final_epochs = 10 # You can adjust this based on dataset size and complexity\n",
        "print(f\"Training final model for {final_epochs} epochs with batch size {best_params['batch_size']}\")\n",
        "final_history = final_model.fit(x_train_combined, y_train_combined,\n",
        "                                batch_size=best_params[\"batch_size\"],\n",
        "                                epochs=final_epochs,\n",
        "                                verbose=1) # Show verbose output for final training\n",
        "\n",
        "# --- 7. Evaluate with Test Set ---\n",
        "print(\"\\n--- 7. Evaluating the final model on the test set ---\")\n",
        "test_loss, test_accuracy = final_model.evaluate(x_test, y_test, verbose=1)\n",
        "\n",
        "print(f\"\\nFinal Model Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Final Model Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "print(\"\\n--- Workflow Complete ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV20Sa4mGKZ-",
        "outputId": "0d71b0fe-8f54-44a2-d671-bbaa7c138095"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.18.0\n",
            "--- 1. Loading and preparing the MNIST dataset ---\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training data shape: (50000, 28, 28, 1), labels shape: (50000,)\n",
            "Validation data shape: (10000, 28, 28, 1), labels shape: (10000,)\n",
            "Test data shape: (10000, 28, 28, 1), labels shape: (10000,)\n",
            "\n",
            "--- 3. Training multiple models with different hyperparameters ---\n",
            "\n",
            "--- Training Model 1/4 ---\n",
            "Hyperparameters: {'learning_rate': 0.01, 'batch_size': 32, 'epochs': 5}\n",
            "--- 4. Evaluating Model 1 on the validation set ---\n",
            "Validation Accuracy: 0.9894, Validation Loss: 0.0456\n",
            "\n",
            "--- Training Model 2/4 ---\n",
            "Hyperparameters: {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 5}\n",
            "--- 4. Evaluating Model 2 on the validation set ---\n",
            "Validation Accuracy: 0.9906, Validation Loss: 0.0361\n",
            "\n",
            "--- Training Model 3/4 ---\n",
            "Hyperparameters: {'learning_rate': 0.0005, 'batch_size': 64, 'epochs': 5}\n",
            "--- 4. Evaluating Model 3 on the validation set ---\n",
            "Validation Accuracy: 0.9850, Validation Loss: 0.0516\n",
            "\n",
            "--- Training Model 4/4 ---\n",
            "Hyperparameters: {'learning_rate': 0.001, 'batch_size': 64, 'epochs': 5}\n",
            "--- 4. Evaluating Model 4 on the validation set ---\n",
            "Validation Accuracy: 0.9879, Validation Loss: 0.0400\n",
            "\n",
            "--- 5. Selecting the best model ---\n",
            "Best Model Index: 2\n",
            "Best Validation Accuracy: 0.9906\n",
            "Best Hyperparameters: {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 5}\n",
            "\n",
            "--- 6. Retraining the best model on the combined train and validation set ---\n",
            "Combined training data shape: (60000, 28, 28, 1), labels shape: (60000,)\n",
            "Training final model for 10 epochs with batch size 32\n",
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 26ms/step - accuracy: 0.8596 - loss: 0.4506\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 26ms/step - accuracy: 0.9754 - loss: 0.0844\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 30ms/step - accuracy: 0.9807 - loss: 0.0656\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - accuracy: 0.9842 - loss: 0.0520\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 30ms/step - accuracy: 0.9863 - loss: 0.0464\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - accuracy: 0.9872 - loss: 0.0434\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 30ms/step - accuracy: 0.9888 - loss: 0.0390\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 30ms/step - accuracy: 0.9884 - loss: 0.0370\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - accuracy: 0.9884 - loss: 0.0346\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - accuracy: 0.9893 - loss: 0.0330\n",
            "\n",
            "--- 7. Evaluating the final model on the test set ---\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9895 - loss: 0.0331\n",
            "\n",
            "Final Model Test Accuracy: 0.9911\n",
            "Final Model Test Loss: 0.0279\n",
            "\n",
            "--- Workflow Complete ---\n"
          ]
        }
      ]
    }
  ]
}